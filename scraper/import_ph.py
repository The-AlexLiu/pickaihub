import json
import os
import random
from supabase import create_client, Client
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
# dotenv_path éœ€è¦æŒ‡å‘ .env.local çš„ç»å¯¹è·¯å¾„æˆ–æ­£ç¡®ç›¸å¯¹è·¯å¾„
# å‡è®¾è„šæœ¬åœ¨ pickaihub/scraper/ ä¸‹ï¼Œ.env.local åœ¨ pickaihub/ ä¸‹
load_dotenv(dotenv_path=os.path.join(os.path.dirname(os.path.dirname(__file__)), ".env.local"))

SUPABASE_URL = os.getenv("NEXT_PUBLIC_SUPABASE_URL")
SUPABASE_KEY = os.getenv("NEXT_PUBLIC_SUPABASE_ANON_KEY")
SERVICE_KEY = os.getenv("SUPABASE_SERVICE_ROLE_KEY")

# å¦‚æœæœ‰ Service Role Keyï¼Œä¼˜å…ˆä½¿ç”¨å®ƒä»¥ç»•è¿‡ RLS
client_key = SERVICE_KEY if SERVICE_KEY else SUPABASE_KEY

if not SUPABASE_URL or not client_key:
    print("âŒ Error: Missing Supabase credentials in .env.local")
    exit(1)

supabase: Client = create_client(SUPABASE_URL, client_key)

# æ˜ å°„ ProductHunt Topics åˆ°æˆ‘ä»¬è‡ªå·±çš„ Categories
CATEGORY_MAPPING = {
    "Design Tools": "image",
    "Productivity": "productivity",
    "Artificial Intelligence": "chatbot",
    "Developer Tools": "code",
    "Marketing": "marketing",
    "Writing": "text",
    "Video Editing": "video",
    "Audio": "audio",
    "Business": "business",
    "Finance": "finance",
    "Education": "education",
    "3D": "3d"
}

def map_category(ph_category):
    for key, value in CATEGORY_MAPPING.items():
        if key.lower() in ph_category.lower():
            return value
    return "other"

def import_tools():
    try:
        with open("ai_tools_data.json", "r") as f:
            tools = json.load(f)
    except FileNotFoundError:
        print("âŒ Error: ai_tools_data.json not found. Run the scraper first.")
        return

    print(f"ğŸ“¦ Importing {len(tools)} tools into Supabase...")

    success_count = 0
    skip_count = 0

    for tool in tools:
        # 1. æ•°æ®æ¸…æ´—ä¸æ˜ å°„
        db_tool = {
            "name": tool["name"],
            "description": tool["description"],
            "url": tool["url"],
            "logo": tool["logo"],
            "category": map_category(tool["category"]),
            "tags": tool["tags"],
            "pricing": tool["pricing"],
            "visits": tool["visits"],
            "rating": tool["rating"],
            "is_new": tool["is_new"],
            "is_trending": tool["is_trending"],
            "launch_date": "now()",
            # New fields
            "features": tool.get("features", []),
            "screenshots": tool.get("screenshots", []),
            "price_detail": tool.get("price_detail", "")
        }

        # 2. æ’å…¥æ•°æ®åº“ (Upsert based on name or url to avoid duplicates)
        # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬ç®€å•åœ°ç”¨ name ä½œä¸ºå”¯ä¸€é”®æ£€æŸ¥ï¼Œå®é™…ç”Ÿäº§ç¯å¢ƒå¯èƒ½éœ€è¦æ›´å¤æ‚çš„å»é‡é€»è¾‘
        try:
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
            existing = supabase.table("tools").select("id").eq("name", db_tool["name"]).execute()
            
            if existing.data:
                print(f"âš ï¸ Skipping duplicate: {db_tool['name']}")
                skip_count += 1
                continue
            
            # æ’å…¥æ–°æ•°æ®
            result = supabase.table("tools").insert(db_tool).execute()
            if result.data:
                print(f"âœ… Imported: {db_tool['name']}")
                success_count += 1
                
        except Exception as e:
            print(f"âŒ Error importing {db_tool['name']}: {e}")

    print(f"\nğŸ‰ Import Complete!")
    print(f"âœ… Success: {success_count}")
    print(f"âš ï¸ Skipped: {skip_count}")

if __name__ == "__main__":
    import_tools()
